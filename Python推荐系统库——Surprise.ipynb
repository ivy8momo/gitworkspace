{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic cross-validation 交叉验证与SVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ml-100k could not be found. Do you want to download it? [Y/n] y\n",
      "Trying to download dataset from http://files.grouplens.org/datasets/movielens/ml-100k.zip...\n",
      "Done! Dataset ml-100k has been saved to C:\\Users\\hey/.surprise_data/ml-100k\n",
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9349  0.9388  0.9441  0.9333  0.9369  0.9376  0.0037  \n",
      "MAE (testset)     0.7372  0.7406  0.7437  0.7354  0.7370  0.7388  0.0030  \n",
      "Fit time          6.93    6.87    6.86    6.86    7.51    7.01    0.25    \n",
      "Test time         0.84    0.17    0.22    0.23    0.23    0.34    0.25    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': (6.930888652801514,\n",
       "  6.867884635925293,\n",
       "  6.862859010696411,\n",
       "  6.860879421234131,\n",
       "  7.513324975967407),\n",
       " 'test_mae': array([0.73722038, 0.74055314, 0.74370748, 0.73536637, 0.73697778]),\n",
       " 'test_rmse': array([0.93488139, 0.93883547, 0.94410307, 0.93330198, 0.93689337]),\n",
       " 'test_time': (0.8420693874359131,\n",
       "  0.17210149765014648,\n",
       "  0.21917152404785156,\n",
       "  0.22614002227783203,\n",
       "  0.2251572608947754)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the movielens-100k dataset (download it if needed),\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# We'll use the famous SVD algorithm.\n",
    "algo = SVD()\n",
    "\n",
    "# Run 5-fold cross-validation and print results\n",
    "cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split and the fit() method 设置训练与测试SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9427343662364416"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# Load the movielens-100k dataset (download it if needed),\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# sample random trainset and testset\n",
    "# test set is made of 25% of the ratings.\n",
    "trainset, testset = train_test_split(data, test_size=.25)\n",
    "\n",
    "# We'll use the famous SVD algorithm.\n",
    "algo = SVD()\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on a whole trainset and the predict() method 用全部数据建模KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBasic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.knns.KNNBasic at 0x1f3385f1048>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the movielens-100k dataset\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# Retrieve the trainset.\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "# Build an algorithm, and train it.\n",
    "algo = KNNBasic()\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 196        item: 302        r_ui = 4.00   est = 4.06   {'actual_k': 40, 'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "# Let’s say you’re interested in user 196 and item 302 (make sure they’re in the trainset!)\n",
    "# and you know that the true rating rui=4\n",
    "# We can now predict ratings by directly calling the predict() method\n",
    "\n",
    "uid = str(196)  # raw user id (as in the ratings file). They are **strings**!\n",
    "iid = str(302)  # raw item id (as in the ratings file). They are **strings**!\n",
    "\n",
    "# get a prediction for specific users and items.\n",
    "pred = algo.predict(uid, iid, r_ui=4, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a custom dataset 使用自定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import BaselineOnly\n",
    "from surprise import Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Evaluating RMSE, MAE of algorithm BaselineOnly on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9482  0.9545  0.9403  0.9398  0.9365  0.9439  0.0066  \n",
      "MAE (testset)     0.7523  0.7547  0.7447  0.7453  0.7434  0.7481  0.0045  \n",
      "Fit time          0.27    0.28    0.25    0.26    0.26    0.26    0.01    \n",
      "Test time         0.15    0.25    0.14    0.19    0.20    0.19    0.04    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': (0.27380967140197754,\n",
       "  0.2751924991607666,\n",
       "  0.25318241119384766,\n",
       "  0.25818800926208496,\n",
       "  0.2581608295440674),\n",
       " 'test_mae': array([0.75226501, 0.75471259, 0.74472754, 0.74527895, 0.74340791]),\n",
       " 'test_rmse': array([0.94818966, 0.9544741 , 0.94031428, 0.93977826, 0.9364955 ]),\n",
       " 'test_time': (0.14783310890197754,\n",
       "  0.253706693649292,\n",
       "  0.14209747314453125,\n",
       "  0.190138578414917,\n",
       "  0.20316433906555176)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path to dataset file\n",
    "file_path = os.path.expanduser(\"C:\\\\Users\\\\hey\\\\.surprise_data\\\\ml-100k\\\\ml-100k\\\\u.data\")\n",
    "\n",
    "# As we're loading a custom dataset, we need to define a reader. \n",
    "# 在加载数据集之前需要初始化一个reader，因为加载本地方法需要两个参数，一个是你的数据集的地址，另一个就是初始化一个Reader对象\n",
    "# In the movielens-100k dataset, each line has the following format:\n",
    "# 'user item rating timestamp', separated by '\\t' characters.\n",
    "# 定义Reader类分割文件后，导入的本地文件的数据结构必须为：user ;item ;rating ; [timestamp]格式，\n",
    "# 当然你可以少个timestamp也是没关系的，user为用户的id；item为项目的id；rating为项目所在用户id的评分；\n",
    "# line_format为数据的行格式，也就是上面的user ; item ; rating ;而seq的意思是要去怎么分割行数据，比如说根据空格或者逗号\n",
    "\n",
    "reader = Reader(line_format='user item rating timestamp', sep='\\t')\n",
    "\n",
    "data = Dataset.load_from_file(file_path, reader=reader)\n",
    "\n",
    "# We can now use this dataset as we please, e.g. calling cross_validate\n",
    "cross_validate(BaselineOnly(), data, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from surprise import NormalPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': (0.0, 0.001001596450805664),\n",
       " 'test_mae': array([1.29247703, 0.88441249]),\n",
       " 'test_rmse': array([1.52467677, 0.89469998]),\n",
       " 'test_time': (0.02602076530456543, 0.0)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creation of the dataframe. Column names are irrelevant.\n",
    "ratings_dict = {'itemID': [1, 1, 1, 2, 2],\n",
    "                'userID': [9, 32, 2, 45, 'user_foo'],\n",
    "                'rating': [3, 2, 4, 3, 1]}\n",
    "df = pd.DataFrame(ratings_dict)\n",
    "\n",
    "# A reader is still needed but only the rating_scale param is requiered.\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(df[['userID', 'itemID', 'rating']], reader)\n",
    "\n",
    "# We can now use this dataset as we please, e.g. calling cross_validate\n",
    "cross_validate(NormalPredictor(), data, cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "      <th>userID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>user_foo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   itemID  rating    userID\n",
       "0       1       3         9\n",
       "1       1       2        32\n",
       "2       1       4         2\n",
       "3       2       3        45\n",
       "4       2       1  user_foo"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use cross-validation iterators 交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9418\n",
      "RMSE: 0.9474\n",
      "RMSE: 0.9468\n"
     ]
    }
   ],
   "source": [
    "# 对完整数据集自动循环进行拆分，交叉验证\n",
    "# Load the movielens-100k dataset\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# define a cross-validation iterator\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "algo = SVD()\n",
    "\n",
    "for trainset, testset in kf.split(data):\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    accuracy.rmse(predictions, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对已经拆分好的数据集，分别读取\n",
    "\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import PredefinedKFold\n",
    "\n",
    "# path to dataset folder\n",
    "files_dir = os.path.expanduser('~/.surprise_data/ml-100k/ml-100k/')\n",
    "\n",
    "# This time, we'll use the built-in reader.\n",
    "reader = Reader('ml-100k')\n",
    "\n",
    "# folds_files is a list of tuples containing file paths:\n",
    "# [(u1.base, u1.test), (u2.base, u2.test), ... (u5.base, u5.test)]\n",
    "train_file = files_dir + 'u%d.base'\n",
    "test_file = files_dir + 'u%d.test'\n",
    "folds_files = [(train_file % i, test_file % i) for i in (1, 2, 3, 4, 5)]\n",
    "\n",
    "data = Dataset.load_from_folds(folds_files, reader=reader)\n",
    "pkf = PredefinedKFold()\n",
    "\n",
    "algo = SVD()\n",
    "\n",
    "for trainset, testset in pkf.split(data):\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    accuracy.rmse(predictions, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune algorithm parameters with GridSearchCV 算法参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9642954850162692\n",
      "{'n_epochs': 10, 'lr_all': 0.005, 'reg_all': 0.4}\n"
     ]
    }
   ],
   "source": [
    "# Use movielens-100K\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# 定义好需要优选的参数网格\n",
    "param_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],\n",
    "              'reg_all': [0.4, 0.6]}\n",
    "\n",
    "# 使用网格搜索交叉验证\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "# best RMSE score\n",
    "print(gs.best_score['rmse'])\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x2f4306e0ef0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now use the algorithm that yields the best rmse:\n",
    "algo = gs.best_estimator['rmse']\n",
    "algo.fit(data.build_full_trainset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
