{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15 电商产品评论数据分析 - jieba分词 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import jieba\n",
    "\n",
    "os.chdir('E:\\Jupyterspace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读入数据\n",
    "inputfile1 = 'meidi_jd_neg.txt'\n",
    "inputfile2 = 'meidi_jd_pos.txt'\n",
    "data1 = pd.read_csv(inputfile1, encoding = 'utf-8', header = None) \n",
    "data2 = pd.read_csv(inputfile2, encoding = 'utf-8', header = None)\n",
    "\n",
    "\n",
    "#通过“广播”形式,jieba分词，加快速度\n",
    "mycut = lambda s: ' '.join(jieba.cut(s)) #自定义简单分词函数\n",
    "data1 = data1[0].apply(mycut) \n",
    "data2 = data2[0].apply(mycut)\n",
    "\n",
    "\n",
    "#保存分词结果\n",
    "outputfile1 = 'meidi_jd_neg_cut.txt'\n",
    "outputfile2 = 'meidi_jd_pos_cut.txt'\n",
    "data1.to_csv(outputfile1, index = False, header = False, encoding = 'utf-8') \n",
    "data2.to_csv(outputfile2, index = False, header = False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15 电商产品评论数据 - LDA主题模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim告警不显示\n",
    "import warnings\n",
    "from gensim import corpora, models\n",
    "warnings.filterwarnings(action='ignore',category=UserWarning,module='gensim')\n",
    "\n",
    "\n",
    "# 读入分词数据和停用词表\n",
    "negfile = 'meidi_jd_neg_cut.txt'\n",
    "posfile = 'meidi_jd_pos_cut.txt'\n",
    "stoplist = 'stoplist.txt'\n",
    "neg = pd.read_csv(negfile, encoding = 'utf-8', header = None)\n",
    "pos = pd.read_csv(posfile, encoding = 'utf-8', header = None)\n",
    "stop = pd.read_csv(stoplist, encoding = 'utf-8', header = None, sep = 'tipdm',engine='python') #sep设置一个不存在的分割词tipdm，因为csv中的逗号与停用词重复，\n",
    "stop = [' ', ''] + list(stop[0]) #Pandas自动过滤了空格符，这里手动添加\n",
    "\n",
    "\n",
    "# 停用词剔除\n",
    "neg[1] = neg[0].apply(lambda s: s.split(' ')) #定义一个分割函数，然后用apply广播\n",
    "neg[2] = neg[1].apply(lambda x: [i for i in x if i not in stop]) #逐词判断是否停用词，思路同上\n",
    "pos[1] = pos[0].apply(lambda s: s.split(' '))\n",
    "pos[2] = pos[1].apply(lambda x: [i for i in x if i not in stop])\n",
    "\n",
    "\n",
    "#负面主题分析\n",
    "neg_dict = corpora.Dictionary(neg[2]) #建立词典\n",
    "neg_corpus = [neg_dict.doc2bow(i) for i in neg[2]] #建立语料库\n",
    "neg_lda = models.LdaModel(neg_corpus, num_topics = 3, id2word = neg_dict) #LDA模型训练\n",
    "#正面主题分析\n",
    "pos_dict = corpora.Dictionary(pos[2])\n",
    "pos_corpus = [pos_dict.doc2bow(i) for i in pos[2]]\n",
    "pos_lda = models.LdaModel(pos_corpus, num_topics = 3, id2word = pos_dict)\n",
    "\n",
    "# 查看结果\n",
    "pos_lda.print_topic(1)\n",
    "neg_lda.print_topic(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15 电商产品评论数据 - TFIDF模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\hey\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.436 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 简单样例1\n",
    "import jieba\n",
    "from gensim import corpora,models,similarities\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore',category=UserWarning,module='gensim')\n",
    "\n",
    "# 语料 all_doc\n",
    "doc0 = \"我不喜欢上海\"\n",
    "doc1 = \"上海是一个好地方\"\n",
    "doc2 = \"北京是一个好地方\"\n",
    "doc3 = \"上海好吃的在哪里\"\n",
    "doc4 = \"上海好玩的在哪里\"\n",
    "doc5 = \"上海是好地方\"\n",
    "doc6 = \"上海路和上海人\"\n",
    "doc7 = \"喜欢小吃\"\n",
    "all_doc = []\n",
    "all_doc.append(doc0)\n",
    "all_doc.append(doc1)\n",
    "all_doc.append(doc2)\n",
    "all_doc.append(doc3)\n",
    "all_doc.append(doc4)\n",
    "all_doc.append(doc5)\n",
    "all_doc.append(doc6)\n",
    "all_doc.append(doc7)\n",
    "\n",
    "\n",
    "# 分词\n",
    "all_doc_list = []\n",
    "for doc in all_doc:\n",
    "    doc_list = [word for word in jieba.cut(doc)]\n",
    "    all_doc_list.append(doc_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单样例2\n",
    "import jieba.posseg as pseg\n",
    "import codecs\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "\n",
    "import os\n",
    "os.chdir('E:\\Jupyterspace')\n",
    "# 读入停用词表\n",
    "stop_words = 'stoplist.txt'\n",
    "stopwords = codecs.open(stop_words,'r',encoding='utf8').readlines() \n",
    "stopwords = [ w.strip() for w in stopwords ]\n",
    "\n",
    "# 结巴分词后的停用词性 [标点符号、连词、助词、副词、介词、时语素、‘的’、数词、方位词、代词]\n",
    "stop_flag = ['x', 'c', 'u','d', 'p', 't', 'uj', 'm', 'f', 'r']\n",
    "\n",
    "# jieba分词，去停用词，效率低，参考使用上一种广播方法\n",
    "def tokenization(filename): \n",
    "    result = [] \n",
    "    with open(filename, 'r',encoding='utf8') as f: \n",
    "        text = f.read() \n",
    "        words = pseg.cut(text) \n",
    "    for word, flag in words: \n",
    "        if flag not in stop_flag and word not in stopwords: \n",
    "            result.append(word) \n",
    "    return result\n",
    "\n",
    "filenames = ['帮您稳血压.txt','脱脂奶.txt','ios.txt']            \n",
    "corpus = []\n",
    "for each in filenames:\n",
    "    corpus.append(tokenization(each))\n",
    "\n",
    "# 训练tf-idf模型\n",
    "dictionary = corpora.Dictionary(corpus)  \n",
    "doc_vectors = [dictionary.doc2bow(text) for text in corpus]  \n",
    "tfidf = models.TfidfModel(doc_vectors)  \n",
    "tfidf_vectors = tfidf[doc_vectors] #  \n",
    "index = similarities.MatrixSimilarity(tfidf_vectors)\n",
    "\n",
    "\n",
    "# 计算测试数据相似性\n",
    "query = tokenization('降压药.txt')\n",
    "query_bow = dictionary.doc2bow(query)\n",
    "sims = index[query_bow]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "import pandas as pd\n",
    "inputfile = 'huizong.csv' #评论汇总文件\n",
    "data = pd.read_csv(inputfile, encoding = 'utf-8')\n",
    "data1 = data[[u'评论']][data[u'品牌'] == u'美的']\n",
    "data2 = data[[u'评论']][data[u'品牌'] == u'海尔']\n",
    "data3 = data[[u'评论']][data[u'品牌'] == u'万和']\n",
    "test = data[[u'评论']][data[u'品牌'] == u'万家乐']\n",
    "# 去重\n",
    "data1 = data1.drop_duplicates(keep='first')\n",
    "data2 = data2.drop_duplicates(keep='first')\n",
    "data3 = data3.drop_duplicates(keep='first')\n",
    "test = test.drop_duplicates(keep='first')\n",
    "\n",
    "# 保存数据\n",
    "filenames = ['meidi_jd.txt','haier_jd.txt','wanhe_jd.txt','wanjiale_jd.txt']\n",
    "data1.to_csv(filenames[0], index = False, header = False, encoding = 'utf-8')\n",
    "data2.to_csv(filenames[1], index = False, header = False, encoding = 'utf-8')\n",
    "data3.to_csv(filenames[2], index = False, header = False, encoding = 'utf-8')\n",
    "test.to_csv(filenames[3], index = False, header = False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读入数据\n",
    "inputfile1 = 'meidi_jd_neg.txt'\n",
    "inputfile2 = 'meidi_jd_pos.txt'\n",
    "data1 = pd.read_csv(inputfile1, encoding = 'utf-8', header = None) \n",
    "data2 = pd.read_csv(inputfile2, encoding = 'utf-8', header = None)\n",
    "\n",
    "\n",
    "#通过“广播”形式,jieba分词，加快速度\n",
    "mycut = lambda s: ' '.join(jieba.cut(s)) #自定义简单分词函数\n",
    "data1 = data1[0].apply(mycut) \n",
    "data2 = data2[0].apply(mycut)\n",
    "\n",
    "\n",
    "# 读入分词数据和停用词表\n",
    "negfile = 'meidi_jd_neg_cut.txt'\n",
    "posfile = 'meidi_jd_pos_cut.txt'\n",
    "stoplist = 'stoplist.txt'\n",
    "neg = pd.read_csv(negfile, encoding = 'utf-8', header = None)\n",
    "pos = pd.read_csv(posfile, encoding = 'utf-8', header = None)\n",
    "stop = pd.read_csv(stoplist, encoding = 'utf-8', header = None, sep = 'tipdm',engine='python') #sep设置一个不存在的分割词tipdm，因为csv中的逗号与停用词重复，\n",
    "stop = [' ', ''] + list(stop[0]) #Pandas自动过滤了空格符，这里手动添加\n",
    "\n",
    "\n",
    "# 停用词剔除\n",
    "neg[1] = neg[0].apply(lambda s: s.split(' ')) #定义一个分割函数，然后用apply广播\n",
    "neg[2] = neg[1].apply(lambda x: [i for i in x if i not in stop]) #逐词判断是否停用词，思路同上\n",
    "pos[1] = pos[0].apply(lambda s: s.split(' '))\n",
    "pos[2] = pos[1].apply(lambda x: [i for i in x if i not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词\n",
    "all_doc_list = []\n",
    "for doc in all_doc:\n",
    "    doc_list = [word for word in jieba.cut(doc)]\n",
    "    all_doc_list.append(doc_list)\n",
    "    \n",
    "\n",
    "# 获取词袋\n",
    "dictionary = corpora.Dictionary(all_doc_list) \n",
    "\n",
    "\n",
    "# 制作语料库\n",
    "corpus = [dictionary.doc2bow(doc) for doc in all_doc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['我', '不', '喜欢', '上海'],\n",
       " ['上海', '是', '一个', '好', '地方'],\n",
       " ['北京', '是', '一个', '好', '地方'],\n",
       " ['上海', '好吃', '的', '在', '哪里'],\n",
       " ['上海', '好玩', '的', '在', '哪里'],\n",
       " ['上海', '是', '好', '地方'],\n",
       " ['上海', '路', '和', '上海', '人'],\n",
       " ['喜欢', '小吃']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########dictionary信息##########\n",
      "Dictionary(18 unique tokens: ['上海', '不', '喜欢', '我', '一个']...)\n",
      "##字典，{单词id，在多少文档中出现}\n",
      "{3: 1, 1: 1, 2: 2, 0: 6, 7: 3, 4: 2, 6: 3, 5: 3, 8: 1, 11: 1, 12: 2, 10: 2, 9: 2, 13: 1, 16: 1, 15: 1, 14: 1, 17: 1}\n",
      "##文档数目\n",
      "8\n",
      "##字典，{单词id，对应的词}\n",
      "{0: '上海', 1: '不', 2: '喜欢', 3: '我', 4: '一个', 5: '地方', 6: '好', 7: '是', 8: '北京', 9: '哪里', 10: '在', 11: '好吃', 12: '的', 13: '好玩', 14: '人', 15: '和', 16: '路', 17: '小吃'}\n",
      "##所有词的个数\n",
      "35\n",
      "##每个文件中不重复词个数的和\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"########dictionary信息##########\")\n",
    "print(str(dictionary))\n",
    "print (\"##字典，{单词id，在多少文档中出现}\")\n",
    "print (dictionary.dfs) #字典，{单词id，在多少文档中出现}\n",
    "print (\"##文档数目\")\n",
    "print (dictionary.num_docs) #文档数目\n",
    "print (\"##字典，{单词id，对应的词}\")\n",
    "print(dict(dictionary.iteritems()))\n",
    "print (\"##所有词的个数\")\n",
    "print (dictionary.num_pos) #所有词的个数\n",
    "print (\"##每个文件中不重复词个数的和\")\n",
    "print (dictionary.num_nnz) #每个文件中不重复词个数的和\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in all_doc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词袋，列表[(单词id，词频)]\n",
      "[['我', '不', '喜欢', '上海'], ['上海', '是', '一个', '好', '地方'], ['北京', '是', '一个', '好', '地方'], ['上海', '好吃', '的', '在', '哪里'], ['上海', '好玩', '的', '在', '哪里'], ['上海', '是', '好', '地方'], ['上海', '路', '和', '上海', '人'], ['喜欢', '小吃']]\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1)], [(0, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(4, 1), (5, 1), (6, 1), (7, 1), (8, 1)], [(0, 1), (9, 1), (10, 1), (11, 1), (12, 1)], [(0, 1), (9, 1), (10, 1), (12, 1), (13, 1)], [(0, 1), (5, 1), (6, 1), (7, 1)], [(0, 2), (14, 1), (15, 1), (16, 1)], [(2, 1), (17, 1)]]\n"
     ]
    }
   ],
   "source": [
    "print (\"词袋，列表[(单词id，词频)]\")\n",
    "print(all_doc_list)\n",
    "print(corpus) # 每一条语料中包含单词id及其在该语料中的词频\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-d905198fec01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mli\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mli\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid2token\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "li = corpus.item()\n",
    "for id, freq in li:\n",
    "    print (id, dictionary.id2token[id], freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过“广播”形式,jieba分词，加快速度\n",
    "import pandas as pd\n",
    "newdata = pd.DataFrame(all_doc)\n",
    "mycut = lambda s: ' '.join(jieba.cut(s)) #自定义简单分词函数\n",
    "newdata = newdata[0].apply(mycut) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "doc_test=\"我喜欢上海的小吃\"\n",
    "doc_test_list = [word for word in jieba.cut(doc_test)]\n",
    "doc_test_vec = dictionary.doc2bow(doc_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf\n",
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我', '喜欢', '上海', '的', '小吃']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.08112725037593049),\n",
       " (2, 0.3909393754390612),\n",
       " (3, 0.5864090631585919),\n",
       " (12, 0.3909393754390612),\n",
       " (17, 0.5864090631585919)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf[doc_test_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 0.70477605),\n",
       " (0, 0.54680777),\n",
       " (3, 0.17724207),\n",
       " (4, 0.17724207),\n",
       " (5, 0.013545224),\n",
       " (6, 0.01279765),\n",
       " (1, 0.010553493),\n",
       " (2, 0.0)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=len(dictionary.keys()))\n",
    "sim = index[tfidf[doc_test_vec]]\n",
    "sorted(enumerate(sim), key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['我', '不', '喜欢', '上海'],\n",
       " ['上海', '是', '一个', '好', '地方'],\n",
       " ['北京', '是', '一个', '好', '地方'],\n",
       " ['上海', '好吃', '的', '在', '哪里'],\n",
       " ['上海', '好玩', '的', '在', '哪里'],\n",
       " ['上海', '是', '好', '地方'],\n",
       " ['上海', '路', '和', '上海', '人'],\n",
       " ['喜欢', '小吃']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(all_doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import jieba, os\n",
    "\n",
    "import codecs\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "import importlib\n",
    "\n",
    "importlib.reload(sys)\n",
    "\n",
    " \n",
    "\n",
    "def print_dict(dict):\n",
    "\n",
    "    for key in dict:\n",
    "\n",
    "        print (type(key),key,str(dict[key])),\n",
    "\n",
    "    print\n",
    "\n",
    "def test3():\n",
    "\n",
    "    '''''\n",
    "\n",
    "    gensim学习之Dictionary\n",
    "\n",
    "    '''\n",
    "\n",
    "    a = [['一','一','二'],['一','二','三']]\n",
    "\n",
    "    b = ['一','一','三','四','四']\n",
    "\n",
    "    dictionary = corpora.Dictionary(a)\n",
    "\n",
    "    print (\"########dictionary信息##########\")\n",
    "\n",
    "    print (str(dictionary)) #\n",
    "\n",
    "    print (\"字典，{单词id，在多少文档中出现}\")\n",
    "\n",
    "    print (dictionary.dfs) #字典，{单词id，在多少文档中出现}\n",
    "\n",
    "    print (\"文档数目\")\n",
    "\n",
    "    print (dictionary.num_docs) #文档数目\n",
    "\n",
    "    print (\"dictionary.items()\")\n",
    "\n",
    "    print_dict(dict(dictionary.items())) #\n",
    "\n",
    "    print (\"字典，{单词id，对应的词}\")\n",
    "\n",
    "    print_dict(dictionary.id2token) #字典，{单词id，对应的词}\n",
    "\n",
    "    print (\"字典，{词，对应的单词id}\")\n",
    "\n",
    "    print_dict(dictionary.token2id) #字典，{词，对应的单词id}\n",
    "\n",
    "    print (\"所有词的个数\")\n",
    "\n",
    "    print (dictionary.num_pos) #所有词的个数\n",
    "\n",
    "    print (\"每个文件中不重复词个数的和\")\n",
    "\n",
    "    print (dictionary.num_nnz) #每个文件中不重复词个数的和\n",
    "\n",
    "    print (\"########doc2bow##########\")\n",
    "\n",
    "    #dictionary.add_documents([b])\n",
    "\n",
    "    #allow_update->更新当前字典；return_missing->返回字典中不存在的词\n",
    "\n",
    "    #result为b文章转换得到的词袋，列表[(单词id，词频)]\n",
    "\n",
    "    result, missing = dictionary.doc2bow(b, allow_update=False, return_missing=True)\n",
    "\n",
    "    print (\"词袋b，列表[(单词id，词频)]\")\n",
    "\n",
    "    print (result)\n",
    "\n",
    "    print (\"不在字典中的词及其词频，字典[(单词，词频)]\")\n",
    "\n",
    "    print_dict(missing)\n",
    "\n",
    "    print (\"########bow信息##########\")\n",
    "\n",
    "    for id, freq in result:\n",
    "\n",
    "        print (id, dictionary.id2token[id], freq)\n",
    "\n",
    "    print (\"########dictionary信息##########\")\n",
    "\n",
    "    #过滤文档频率大于no_below，小于no_above*num_docs的词\n",
    "\n",
    "    dictionary.filter_extremes(no_below=1, no_above=0.5, keep_n=10)\n",
    "\n",
    " \n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########dictionary信息##########\n",
      "Dictionary(3 unique tokens: ['一', '二', '三'])\n",
      "字典，{单词id，在多少文档中出现}\n",
      "{0: 2, 1: 2, 2: 1}\n",
      "文档数目\n",
      "2\n",
      "dictionary.items()\n",
      "<class 'int'> 0 一\n",
      "<class 'int'> 1 二\n",
      "<class 'int'> 2 三\n",
      "字典，{单词id，对应的词}\n",
      "<class 'int'> 0 一\n",
      "<class 'int'> 1 二\n",
      "<class 'int'> 2 三\n",
      "字典，{词，对应的单词id}\n",
      "<class 'str'> 一 0\n",
      "<class 'str'> 二 1\n",
      "<class 'str'> 三 2\n",
      "所有词的个数\n",
      "6\n",
      "每个文件中不重复词个数的和\n",
      "5\n",
      "########doc2bow##########\n",
      "词袋b，列表[(单词id，词频)]\n",
      "[(0, 2), (2, 1)]\n",
      "不在字典中的词及其词频，字典[(单词，词频)]\n",
      "<class 'str'> 四 2\n",
      "########bow信息##########\n",
      "0 一 2\n",
      "2 三 1\n",
      "########dictionary信息##########\n"
     ]
    }
   ],
   "source": [
    "test3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(18 unique tokens: ['上海', '不', '喜欢', '我', '一个']...)\n"
     ]
    }
   ],
   "source": [
    "print(str(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 6,\n",
       " 1: 1,\n",
       " 2: 2,\n",
       " 3: 1,\n",
       " 4: 2,\n",
       " 5: 3,\n",
       " 6: 3,\n",
       " 7: 3,\n",
       " 8: 1,\n",
       " 9: 2,\n",
       " 10: 2,\n",
       " 11: 1,\n",
       " 12: 2,\n",
       " 13: 1,\n",
       " 14: 1,\n",
       " 15: 1,\n",
       " 16: 1,\n",
       " 17: 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
